<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ibra Niang — GPU stuff</title>

  <meta name="description" content="GPU stuff reading list." />
  <meta name="color-scheme" content="dark" />

  <link rel="stylesheet" href="style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" />
</head>

<body>
  <a class="skip-link" href="#content">Skip to content</a>

  <header class="site-header">
    <nav class="top-nav" aria-label="Primary">
      <a class="top-link" href="index.html">home</a>
      <a class="top-link" href="projects.html">projects</a>
      <a class="top-link" href="blog.html">blog</a>
      <a class="top-link is-active" href="reading.html">reading list</a>
      <a class="top-link" href="school.html">school</a>
    </nav>
  </header>

  <main id="content" class="container">
    <section class="section">
      <h2>GPU stuff</h2>

      <h3>Practical recommendation (what I would do in your shoes)</h3>
      <ul class="clean-list">
        <li>Spend about 1 to 2 weeks on Triton: you will already cover attention + optimizer + baseline GEMM with fast iteration.</li>
        <li>Then move to CuTe DSL specifically for GEMM + epilogues once you want CUTLASS-grade layout control (NVIDIA docs).</li>
        <li>If the kernel is "GEMM is the main event" -> CuTe DSL / CUTLASS.</li>
        <li>If the kernel is "softmax + streaming + attention structure" -> Triton.</li>
      </ul>

      <h3>Starting Sunday</h3>
      <ul class="clean-list">
        <li>Watch CUDA 3 seconds vids from YouTube.</li>
        <li>Twitter guy Elliot.</li>
        <li>Follow GPU notes and inference.</li>
        <li>Check phys NN Twitter.</li>
      </ul>

      <h3>GPU mode competition</h3>
      <ul class="clean-list">
        <li><a href="https://www.gpumode.com/v2/home" target="_blank" rel="noreferrer">GPU mode v2</a></li>
        <li>NVIDIA guidebook</li>
        <li>PPMP</li>
        <li>OpenAI Triton docs</li>
        <li><a href="https://github.com/a-hamdi" target="_blank" rel="noreferrer">CUDA challenges</a></li>
      </ul>

      <h3>Core docs and languages</h3>
      <ul class="clean-list">
        <li><a href="https://triton-lang.org/main/index.html" target="_blank" rel="noreferrer">Triton language</a></li>
        <li><a href="https://docs.nvidia.com/cuda/cuda-programming-guide/part1.html" target="_blank" rel="noreferrer">CUDA programming guide (part 1)</a></li>
      </ul>

      <h3>Notes and links</h3>
      <ul class="clean-list">
        <li><a href="https://gist.github.com/vikhyat/82a7b9d107674c11d94f9fab402283cd" target="_blank" rel="noreferrer">vikhyat GPU notes (gist)</a></li>
        <li><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/" target="_blank" rel="noreferrer">Inference optimization (Lilian Weng)</a></li>
        <li><a href="https://github.com/vikhyat/mixtral-inference/blob/main/mixtral/model.py" target="_blank" rel="noreferrer">Mixtral inference model.py</a></li>
        <li>KernelBench</li>
        <li>FlashMLA</li>
        <li><a href="https://puzzles.modular.com/introduction.html" target="_blank" rel="noreferrer">Modular puzzles</a></li>
        <li><a href="https://github.com/pranjalssh/fast.cu/blob/main/examples/matmul/matmul_8.cuh" target="_blank" rel="noreferrer">fast.cu matmul example</a></li>
        <li><a href="https://docs.google.com/document/u/0/d/1ejsAfmKz_pqn21PSZXYdGo3ATRWJN0DGcPSGVFBsd3U/mobilebasic" target="_blank" rel="noreferrer">Precision notes (doc)</a></li>
        <li><a href="https://www.turingpost.com/p/fp16" target="_blank" rel="noreferrer">FP16 (Turing Post)</a></li>
        <li>Add from scratch_project folder on project.</li>
        <li><a href="https://hipscript.lights0123.com/" target="_blank" rel="noreferrer">Compile CUDA in the browser (HipScript)</a></li>
        <li><a href="https://lights0123.com/blog/2025/01/07/hip-script/" target="_blank" rel="noreferrer">HipScript architecture blog</a></li>
        <li><a href="http://leetgpu.com" target="_blank" rel="noreferrer">LeetGPU</a></li>
        <li><a href="https://t.co/UkKAVcN8Kr" target="_blank" rel="noreferrer">Intro to CUDA on HIPs</a></li>
        <li>Intro from NVIDIA guy.</li>
        <li>Site by Googol lady.</li>
        <li><a href="https://github.com/a-hamdi/GPU" target="_blank" rel="noreferrer">Ref 100d CUDA (GitHub)</a></li>
        <li><a href="https://modal.com/gpu-glossary" target="_blank" rel="noreferrer">Modal GPU architecture</a></li>
        <li>Tristan Hume</li>
        <li>Bohem</li>
        <li>How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog.</li>
        <li><a href="https://michalpitr.substack.com/p/gpu-programming" target="_blank" rel="noreferrer">GPU programming from scratch</a></li>
        <li>CUDA SGEMM optim (wangzyon GitHub repository).</li>
        <li><a href="https://t.co/GCM3oW3S7R" target="_blank" rel="noreferrer">High performance attention mechanism</a></li>
        <li><a href="https://gau-nernst.github.io/amd-a2a/" target="_blank" rel="noreferrer">AMD A2A</a></li>
        <li><a href="https://andrewkchan.dev/posts/yalm.html" target="_blank" rel="noreferrer">Fast LLM inference (YALM)</a></li>
        <li><a href="https://ppc.cs.aalto.fi/" target="_blank" rel="noreferrer">Programming Parallel Computers</a></li>
        <li><a href="https://jax-ml.github.io/scaling-book/gpus/" target="_blank" rel="noreferrer">JAX scaling book: GPUs</a></li>
        <li><a href="https://www.enbao.me/posts/tk" target="_blank" rel="noreferrer">Inside tdkittens</a></li>
        <li><a href="https://cvw.cac.cornell.edu/gpu-architecture" target="_blank" rel="noreferrer">Cornell GPU architecture</a></li>
        <li><a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk" target="_blank" rel="noreferrer">Thinderkittens</a></li>
        <li><a href="https://t.co/4jLomyexEu" target="_blank" rel="noreferrer">Inside NVIDIA GPU</a></li>
        <li><a href="https://t.co/1j7MTnI7rL" target="_blank" rel="noreferrer">CUDA handbook</a></li>
        <li><a href="https://t.co/qn7R1jpVPg" target="_blank" rel="noreferrer">Modal GPU perf</a></li>
        <li><a href="https://docs.jax.dev/en/latest/pallas/gpu/blackwell_matmul.html" target="_blank" rel="noreferrer">JAX high perf kernels (Blackwell matmul)</a></li>
        <li><a href="https://themaister.net/blog/2025/10/05/a-case-for-learning-gpu-programming-with-a-compute-first-mindset/" target="_blank" rel="noreferrer">GPU mindset</a></li>
        <li><a href="https://t.co/70kcTiFYKG" target="_blank" rel="noreferrer">Inside NVIDIA GPU (2)</a></li>
        <li><a href="https://ut21.github.io/blog/triton.html" target="_blank" rel="noreferrer">Kernel graphics (Triton)</a></li>
        <li><a href="https://t.co/O1hWTQp83N" target="_blank" rel="noreferrer">GPU related</a></li>
        <li>Start reading here: <a href="https://open.substack.com/pub/modelcraft/p/fundamentals-of-gpu-engineering" target="_blank" rel="noreferrer">Fundamentals of GPU engineering</a></li>
        <li><a href="https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker" target="_blank" rel="noreferrer">Unsloth Docker images</a></li>
        <li><a href="https://t.co/eE17PeK7Kq" target="_blank" rel="noreferrer">Efficient LLM training</a></li>
        <li><a href="https://t.co/BTvSm7NChH" target="_blank" rel="noreferrer">LLMQ C++ CUDA FSDP multigpu</a></li>
        <li>CUDA thinder.. yt</li>
        <li><a href="https://t.co/fkGrKLx1GO" target="_blank" rel="noreferrer">Flash attention reverse engineering</a></li>
        <li><a href="https://lubits.ch/flash/Part-1" target="_blank" rel="noreferrer">Flash attention from scratch</a></li>
        <li><a href="https://nathanchen.me/public/Flash-Attention-Tilelang.html" target="_blank" rel="noreferrer">Flash attention in Tilelang</a></li>
        <li><a href="http://labs.iximiuz.com/challenges" target="_blank" rel="noreferrer">Linux Docker Kubernetes challenges</a></li>
        <li><a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklo" target="_blank" rel="noreferrer">H100 CUDA matmul kernel from scratch</a></li>
        <li><a href="https://x.com/stuart_sul/status/1990482396232466615?s=46&t=jy3oQDFMLuUygF-hwz084g" target="_blank" rel="noreferrer">Stuart Sul thread</a></li>
        <li><a href="https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction" target="_blank" rel="noreferrer">Modular Magnum series (Blackwell matmul)</a></li>
        <li><a href="https://github.com/stas00/ml-engineering/tree/master" target="_blank" rel="noreferrer">ML engineering (stas00)</a></li>
        <li><a href="https://youtu.be/Pa_l3aHCoGc" target="_blank" rel="noreferrer">Understand CPU (YouTube)</a></li>
        <li><a href="https://ydnyshhh.github.io/posts/flash_infer/" target="_blank" rel="noreferrer">Flash Infer</a></li>
        <li><a href="https://mohitmishra786.github.io/chessman/2024/11/13/Exploring-the-Depths-of-Linux-Input-Output-A-Developer's-Guide-to-Storage-Management.html" target="_blank" rel="noreferrer">Linux kernel IO deep dive</a></li>
      </ul>

      <h3>Some perf related must-reads</h3>
      <ul class="clean-list">
        <li>How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: siboehm.com/articles/22/CU...</li>
        <li>Outperforming cuBLAS on H100: a Worklog: cudaforfun.substack.com/p/outperformin...</li>
        <li>Defeating Nondeterminism in LLM Inference: thinkingmachines.ai/blog/defeating...</li>
        <li>Making Deep Learning go Brrrr From First Principles: horace.io/brrr_intro.html</li>
        <li>Transformer Inference Arithmetic: kipp.ly/transformer-in...</li>
        <li>Domain specific architectures for AI inference: fleetwood.dev/posts/domain-s..</li>
        <li>A postmortem of three recent issues: anthropic.com/engineering/a-...</li>
        <li>How To Scale Your Model: jax-ml.github.io/scaling-book/</li>
        <li>The Ultra-Scale Playbook: huggingface.co/spaces/nanotro...</li>
        <li>The Case for Co-Designing Model Architectures with Hardware: arxiv.org/abs/2401.14489</li>
      </ul>

      <h3>Some recent reads (this month)</h3>
      <ul class="clean-list">
        <li>Inside NVIDIA GPUs: Anatomy of high performance matmul kernels: aleksagordic.com/blog/matmul</li>
        <li>Triton Flash Attention Kernel Walkthrough: The Forward Pass: nathanchen.me/public/Triton-...</li>
        <li>This guy substack: michalpitr.substack.com</li>
        <li>Deep Dive into Triton Internals (3 parts): kapilsharma.dev/posts/deep-div...</li>
        <li>HunyuanWorld-Mirror: Technical Report: 3d-models.hunyuan.tencent.com/world/worldMir...</li>
        <li>Understanding the CUDA Compiler and PTX with a Top-K Kernel: blog.alpindale.net/posts/top_k_cu...</li>
        <li>Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields: arxiv.org/abs/2510.03104</li>
      </ul>

      <h3>From scratch ideas</h3>
      <ul class="clean-list">
        <li>Implement a smol-diloco.</li>
        <li>Mini-FSDP.</li>
        <li>Smol-vLLM (inference engine).</li>
        <li>RL envs (for puzzles like klotski or proving theorems).</li>
        <li>VLA models for robots.</li>
        <li>Dissecting NCCL, CUTLASS, TensorRT, SGLang, vLLM.</li>
        <li>Speed hacks: attention sinks, speculative decoding, quantization, KV-cache tuning, paged attention.</li>
        <li>Experimenting with MoEs: upcycling, expert parallelism, deep EP, router optmzn, DeepSeek-MoE style training.</li>
        <li>Mech interp: CoT probing in reasoning models, exploring how in-context learning emerges, induction heads, comparing distilled and undistilled reasoning models.</li>
        <li>This is a tiny list of things you could build. Do not get stuck going too deep into theory (especially around ML systems, inference, post-training).</li>
      </ul>
    </section>

    <footer class="footer">
      <p class="muted">© 2026 Ibra Niang. All rights reserved.</p>
    </footer>
  </main>

  <button class="back-to-top" id="backToTop" aria-label="Back to top">
    <i class="fas fa-arrow-up"></i>
  </button>

  <script src="app.js" defer></script>
</body>
</html>
